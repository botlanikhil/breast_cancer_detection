{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4441334,"sourceType":"datasetVersion","datasetId":2600743}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import (Input, Dense, Dropout, GlobalAveragePooling2D, \n                                   Concatenate, BatchNormalization, Add, Layer)\nfrom tensorflow.keras.applications import (InceptionV3, EfficientNetB3, ResNet152V2, \n                                         DenseNet169)\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import (EarlyStopping, ReduceLROnPlateau, \n                                      ModelCheckpoint, LearningRateScheduler)\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.utils.class_weight import compute_class_weight\nimport cv2\nfrom tensorflow.keras import mixed_precision\nimport json\n\n# TPU Configuration - CRITICAL FOR TPU USAGE\nprint(\"Setting up TPU...\")\ntry:\n    # Detect TPU\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(f'Running on TPU: {tpu.cluster_spec().as_dict()}')\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_system(tpu)\n    \n    # Create TPU strategy\n    strategy = tf.distribute.TPUStrategy(tpu)\n    print(f\"Number of replicas: {strategy.num_replicas_in_sync}\")\n    \n    # Enable mixed precision for TPU\n    mixed_precision.set_global_policy('mixed_bfloat16')  # Use bfloat16 for TPU\n    \nexcept ValueError:\n    print(\"TPU not found, falling back to GPU/CPU\")\n    # Try GPU\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    if gpus:\n        try:\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, True)\n            strategy = tf.distribute.MirroredStrategy()\n            print(f\"Using GPU with {strategy.num_replicas_in_sync} replicas\")\n            mixed_precision.set_global_policy('mixed_float16')\n        except RuntimeError as e:\n            print(f\"GPU setup failed: {e}\")\n            strategy = tf.distribute.get_strategy()  # Default strategy\n    else:\n        strategy = tf.distribute.get_strategy()  # Default strategy\n        print(\"Using default strategy (CPU)\")\n\n# Set seeds for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# TPU-optimized configuration\nGLOBAL_BATCH_SIZE = 32 * strategy.num_replicas_in_sync  # Scale batch size for TPU\nIMG_SIZE = 224  # Standard size for pre-trained models, TPU-friendly\nEPOCHS = 30\nPATIENCE = 8\n\nprint(f\"Global batch size: {GLOBAL_BATCH_SIZE}\")\nprint(f\"Per replica batch size: {GLOBAL_BATCH_SIZE // strategy.num_replicas_in_sync}\")\n\n# Define data paths - Update these according to your dataset location\ntrain_dir = \"/kaggle/input/ultrasound-breast-images-for-breast-cancer/ultrasound breast classification/train\"\nval_dir = \"/kaggle/input/ultrasound-breast-images-for-breast-cancer/ultrasound breast classification/val\"\n\n# TPU-compatible preprocessing layer\nclass ContrastAdjustmentLayer(Layer):\n    def __init__(self, contrast_factor=1.2, **kwargs):\n        super(ContrastAdjustmentLayer, self).__init__(**kwargs)\n        self.contrast_factor = contrast_factor\n    \n    def call(self, inputs):\n        return tf.image.adjust_contrast(inputs, self.contrast_factor)\n    \n    def get_config(self):\n        config = super().get_config()\n        config.update({\"contrast_factor\": self.contrast_factor})\n        return config\n\n# Enhanced data augmentation optimized for TPU\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=12,\n    width_shift_range=0.08,\n    height_shift_range=0.08,\n    shear_range=0.08,\n    zoom_range=0.12,\n    horizontal_flip=True,\n    brightness_range=[0.85, 1.15],\n    fill_mode='reflect'\n)\n\nval_datagen = ImageDataGenerator(rescale=1./255)\n\n# Data generators with TPU-optimized batch size\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=GLOBAL_BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=True,\n    interpolation='bilinear'\n)\n\nval_generator = val_datagen.flow_from_directory(\n    val_dir,\n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=GLOBAL_BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=False,\n    interpolation='bilinear'\n)\n\nprint(f\"Classes: {list(train_generator.class_indices.keys())}\")\nprint(f\"Training samples: {train_generator.samples}\")\nprint(f\"Validation samples: {val_generator.samples}\")\n\n# Calculate class weights\ny_integers = train_generator.classes\nclass_weights = compute_class_weight(\n    'balanced',\n    classes=np.unique(y_integers),\n    y=y_integers\n)\nclass_weight_dict = dict(enumerate(class_weights))\nprint(f\"Class weights: {class_weight_dict}\")\n\n# TPU-optimized ensemble model\ndef create_tpu_optimized_ensemble_model(input_shape=(IMG_SIZE, IMG_SIZE, 3), num_classes=2):\n    \"\"\"Create TPU-optimized ensemble model\"\"\"\n    \n    # Single input layer to avoid TPU complications\n    input_layer = Input(shape=input_shape, name='main_input')\n    \n    # Apply contrast adjustment\n    contrast_adjusted = ContrastAdjustmentLayer(contrast_factor=1.15, name='contrast_adjustment')(input_layer)\n    \n    # Create base models with proper TPU configuration\n    # EfficientNetB3 - excellent for medical images\n    base1 = EfficientNetB3(\n        include_top=False, \n        weights='imagenet', \n        input_shape=input_shape,\n        pooling=None\n    )\n    base1._name = 'efficientnet_base'\n    \n    # InceptionV3 - multi-scale features\n    base2 = InceptionV3(\n        include_top=False, \n        weights='imagenet', \n        input_shape=input_shape,\n        pooling=None\n    )\n    base2._name = 'inception_base'\n    \n    # ResNet152V2 - deep residual learning\n    base3 = ResNet152V2(\n        include_top=False, \n        weights='imagenet', \n        input_shape=input_shape,\n        pooling=None\n    )\n    base3._name = 'resnet_base'\n    \n    # Initially freeze all base models\n    base_models = [base1, base2, base3]\n    for base in base_models:\n        base.trainable = False\n    \n    # Extract features from each model\n    features1 = base1(contrast_adjusted)\n    features2 = base2(contrast_adjusted)\n    features3 = base3(contrast_adjusted)\n    \n    # Global Average Pooling for each branch\n    gap1 = GlobalAveragePooling2D(name='gap_eff')(features1)\n    gap2 = GlobalAveragePooling2D(name='gap_inc')(features2)\n    gap3 = GlobalAveragePooling2D(name='gap_res')(features3)\n    \n    # Individual feature processing with BatchNorm for TPU stability\n    x1 = BatchNormalization(name='bn1')(gap1)\n    x1 = Dropout(0.3, name='dropout1')(x1)\n    \n    x2 = BatchNormalization(name='bn2')(gap2)\n    x2 = Dropout(0.3, name='dropout2')(x2)\n    \n    x3 = BatchNormalization(name='bn3')(gap3)\n    x3 = Dropout(0.3, name='dropout3')(x3)\n    \n    # Concatenate all features\n    concatenated = Concatenate(name='feature_concat')([x1, x2, x3])\n    \n    # Final classification layers optimized for TPU\n    x = BatchNormalization(name='final_bn')(concatenated)\n    x = Dropout(0.5, name='final_dropout1')(x)\n    x = Dense(512, activation='relu', name='dense1')(x)\n    x = BatchNormalization(name='dense1_bn')(x)\n    x = Dropout(0.4, name='final_dropout2')(x)\n    x = Dense(256, activation='relu', name='dense2')(x)\n    x = BatchNormalization(name='dense2_bn')(x)\n    x = Dropout(0.3, name='final_dropout3')(x)\n    \n    # Output layer with float32 for numerical stability\n    output = Dense(num_classes, activation='softmax', dtype='float32', name='final_output')(x)\n    \n    model = Model(inputs=input_layer, outputs=output, name='tpu_ensemble_breast_cancer_classifier')\n    \n    return model, base_models\n\n# Build model within TPU strategy scope\nprint(\"Building TPU-optimized ensemble model...\")\nwith strategy.scope():\n    model, base_models = create_tpu_optimized_ensemble_model(num_classes=train_generator.num_classes)\n    \n    # Compile model with TPU-optimized settings\n    initial_lr = 0.001 * strategy.num_replicas_in_sync  # Scale learning rate for TPU\n    \n    model.compile(\n        optimizer=Adam(learning_rate=initial_lr, beta_1=0.9, beta_2=0.999, epsilon=1e-7),\n        loss='categorical_crossentropy',\n        metrics=['accuracy', 'precision', 'recall']\n    )\n\nprint(f\"Model built with {model.count_params():,} total parameters\")\nprint(\"Model summary:\")\nmodel.summary()\n\n# TPU-optimized callbacks\ncallbacks = [\n    EarlyStopping(\n        monitor='val_accuracy',\n        patience=PATIENCE,\n        restore_best_weights=True,\n        verbose=1,\n        min_delta=0.001\n    ),\n    ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.3,\n        patience=5,\n        verbose=1,\n        min_lr=1e-8,\n        min_delta=0.0001\n    ),\n    ModelCheckpoint(\n        \"best_tpu_model.h5\",\n        monitor='val_accuracy',\n        save_best_only=True,\n        mode='max',\n        verbose=1,\n        save_weights_only=False\n    )\n]\n\n# Phase 1: Train with frozen base models\nprint(\"\\n\" + \"=\"*60)\nprint(\"PHASE 1: Training with frozen pre-trained models on TPU\")\nprint(\"=\"*60)\n\ntry:\n    history_phase1 = model.fit(\n        train_generator,\n        epochs=12,\n        validation_data=val_generator,\n        callbacks=callbacks,\n        class_weight=class_weight_dict,\n        verbose=1,\n        steps_per_epoch=train_generator.samples // GLOBAL_BATCH_SIZE,\n        validation_steps=val_generator.samples // GLOBAL_BATCH_SIZE\n    )\n    \n    print(\"Phase 1 completed successfully!\")\n    \nexcept Exception as e:\n    print(f\"Error in Phase 1: {e}\")\n    print(\"Trying with smaller batch size...\")\n    \n    # Recreate generators with smaller batch size\n    GLOBAL_BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n    \n    train_generator = train_datagen.flow_from_directory(\n        train_dir,\n        target_size=(IMG_SIZE, IMG_SIZE),\n        batch_size=GLOBAL_BATCH_SIZE,\n        class_mode='categorical',\n        shuffle=True,\n        interpolation='bilinear'\n    )\n    \n    val_generator = val_datagen.flow_from_directory(\n        val_dir,\n        target_size=(IMG_SIZE, IMG_SIZE),\n        batch_size=GLOBAL_BATCH_SIZE,\n        class_mode='categorical',\n        shuffle=False,\n        interpolation='bilinear'\n    )\n    \n    history_phase1 = model.fit(\n        train_generator,\n        epochs=12,\n        validation_data=val_generator,\n        callbacks=callbacks,\n        class_weight=class_weight_dict,\n        verbose=1,\n        steps_per_epoch=train_generator.samples // GLOBAL_BATCH_SIZE,\n        validation_steps=val_generator.samples // GLOBAL_BATCH_SIZE\n    )\n\n# Phase 2: Progressive unfreezing and fine-tuning\nprint(\"\\n\" + \"=\"*60)\nprint(\"PHASE 2: Progressive fine-tuning on TPU\")\nprint(\"=\"*60)\n\ndef progressive_unfreeze_tpu(base_models, unfreeze_percent=0.2):\n    \"\"\"TPU-optimized progressive unfreezing\"\"\"\n    with strategy.scope():\n        for i, base_model in enumerate(base_models):\n            total_layers = len(base_model.layers)\n            unfreeze_from = int(total_layers * (1 - unfreeze_percent))\n            \n            for layer in base_model.layers[unfreeze_from:]:\n                layer.trainable = True\n            \n            print(f\"Base model {i+1}: Unfrozen {total_layers - unfreeze_from}/{total_layers} layers\")\n\n# Progressive unfreezing\nprogressive_unfreeze_tpu(base_models, unfreeze_percent=0.15)\n\n# Recompile with lower learning rate for fine-tuning\nwith strategy.scope():\n    model.compile(\n        optimizer=Adam(learning_rate=1e-5 * strategy.num_replicas_in_sync, beta_1=0.9, beta_2=0.999, epsilon=1e-7),\n        loss='categorical_crossentropy',\n        metrics=['accuracy', 'precision', 'recall']\n    )\n\n# Fine-tuning callbacks\nfine_tune_callbacks = [\n    EarlyStopping(\n        monitor='val_accuracy',\n        patience=6,\n        restore_best_weights=True,\n        verbose=1,\n        min_delta=0.0005\n    ),\n    ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.5,\n        patience=4,\n        verbose=1,\n        min_lr=1e-9\n    ),\n    ModelCheckpoint(\n        \"best_fine_tuned_tpu_model.h5\",\n        monitor='val_accuracy',\n        save_best_only=True,\n        mode='max',\n        verbose=1\n    )\n]\n\nhistory_phase2 = model.fit(\n    train_generator,\n    epochs=18,\n    validation_data=val_generator,\n    callbacks=fine_tune_callbacks,\n    class_weight=class_weight_dict,\n    verbose=1,\n    steps_per_epoch=train_generator.samples // GLOBAL_BATCH_SIZE,\n    validation_steps=val_generator.samples // GLOBAL_BATCH_SIZE\n)\n\n# Final evaluation\nprint(\"\\n\" + \"=\"*60)\nprint(\"FINAL MODEL EVALUATION ON TPU\")\nprint(\"=\"*60)\n\n# Reset generator and predict\nval_generator.reset()\nprint(\"Making predictions...\")\nfinal_predictions = model.predict(\n    val_generator, \n    verbose=1,\n    steps=val_generator.samples // GLOBAL_BATCH_SIZE\n)\ny_pred_final = np.argmax(final_predictions, axis=1)\n\n# Get true labels (truncate to match predictions length)\ny_true_final = val_generator.classes[:len(y_pred_final)]\n\n# Get class names\nclass_names = list(val_generator.class_indices.keys())\n\n# Detailed classification report\nprint(\"\\nFinal Classification Report:\")\nprint(classification_report(y_true_final, y_pred_final, target_names=class_names))\n\n# Calculate metrics\nfinal_accuracy = accuracy_score(y_true_final, y_pred_final)\nfinal_precision = precision_score(y_true_final, y_pred_final, average='weighted')\nfinal_recall = recall_score(y_true_final, y_pred_final, average='weighted')\nfinal_f1 = f1_score(y_true_final, y_pred_final, average='weighted')\n\nprint(f\"\\nFinal Metrics:\")\nprint(f\"Accuracy: {final_accuracy:.4f} ({final_accuracy*100:.2f}%)\")\nprint(f\"Precision: {final_precision:.4f}\")\nprint(f\"Recall: {final_recall:.4f}\")\nprint(f\"F1-Score: {final_f1:.4f}\")\n\n# Confusion matrix\nplt.figure(figsize=(10, 8))\ncm = confusion_matrix(y_true_final, y_pred_final)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n           xticklabels=class_names, yticklabels=class_names)\nplt.title(f'TPU Model Confusion Matrix\\nAccuracy: {final_accuracy:.4f}')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\n\n# TPU-optimized Test-Time Augmentation\ndef tpu_test_time_augmentation(model, generator, n_augmentations=5):\n    \"\"\"TPU-optimized test-time augmentation\"\"\"\n    print(f\"Applying TPU-optimized TTA with {n_augmentations} augmentations...\")\n    \n    all_predictions = []\n    \n    # Original predictions\n    generator.reset()\n    original_preds = model.predict(\n        generator, \n        verbose=0,\n        steps=generator.samples // GLOBAL_BATCH_SIZE\n    )\n    all_predictions.append(original_preds)\n    \n    # Create augmented predictions\n    for aug_idx in range(1, n_augmentations):\n        print(f\"TTA Augmentation {aug_idx + 1}/{n_augmentations}\")\n        \n        # Create new augmented generator\n        aug_datagen = ImageDataGenerator(\n            rescale=1./255,\n            rotation_range=np.random.uniform(5, 15),\n            width_shift_range=np.random.uniform(0.05, 0.1),\n            height_shift_range=np.random.uniform(0.05, 0.1),\n            zoom_range=np.random.uniform(0.05, 0.15),\n            horizontal_flip=True,\n            brightness_range=[0.9, 1.1],\n            fill_mode='reflect'\n        )\n        \n        aug_generator = aug_datagen.flow_from_directory(\n            val_dir,\n            target_size=(IMG_SIZE, IMG_SIZE),\n            batch_size=GLOBAL_BATCH_SIZE,\n            class_mode='categorical',\n            shuffle=False,\n            interpolation='bilinear'\n        )\n        \n        aug_preds = model.predict(\n            aug_generator, \n            verbose=0,\n            steps=aug_generator.samples // GLOBAL_BATCH_SIZE\n        )\n        all_predictions.append(aug_preds)\n    \n    # Average all predictions\n    tta_predictions = np.mean(all_predictions, axis=0)\n    return tta_predictions\n\n# Apply TTA\nprint(\"\\n\" + \"=\"*60)\nprint(\"APPLYING TPU-OPTIMIZED TEST-TIME AUGMENTATION\")\nprint(\"=\"*60)\n\ntta_predictions = tpu_test_time_augmentation(model, val_generator, n_augmentations=4)\ntta_y_pred = np.argmax(tta_predictions, axis=1)\ntta_y_true = val_generator.classes[:len(tta_y_pred)]\n\n# TTA Results\ntta_accuracy = accuracy_score(tta_y_true, tta_y_pred)\ntta_precision = precision_score(tta_y_true, tta_y_pred, average='weighted')\ntta_recall = recall_score(tta_y_true, tta_y_pred, average='weighted')\ntta_f1 = f1_score(tta_y_true, tta_y_pred, average='weighted')\n\nprint(f\"\\nTTA Results:\")\nprint(f\"Accuracy: {tta_accuracy:.4f} ({tta_accuracy*100:.2f}%)\")\nprint(f\"Precision: {tta_precision:.4f}\")\nprint(f\"Recall: {tta_recall:.4f}\")\nprint(f\"F1-Score: {tta_f1:.4f}\")\nprint(f\"Improvement over single model: {((tta_accuracy - final_accuracy) * 100):+.2f}%\")\n\nprint(\"\\nTTA Classification Report:\")\nprint(classification_report(tta_y_true, tta_y_pred, target_names=class_names))\n\n# Performance visualization\nplt.figure(figsize=(15, 10))\n\n# Training history\nplt.subplot(2, 3, 1)\nall_train_acc = []\nall_val_acc = []\n\nfor hist in [history_phase1, history_phase2]:\n    all_train_acc.extend(hist.history['accuracy'])\n    all_val_acc.extend(hist.history['val_accuracy'])\n\nepochs_range = range(1, len(all_train_acc) + 1)\nplt.plot(epochs_range, all_train_acc, 'b-', label='Training Accuracy', alpha=0.8)\nplt.plot(epochs_range, all_val_acc, 'r-', label='Validation Accuracy', alpha=0.8)\nplt.title('TPU Training History')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Loss history\nplt.subplot(2, 3, 2)\nall_train_loss = []\nall_val_loss = []\n\nfor hist in [history_phase1, history_phase2]:\n    all_train_loss.extend(hist.history['loss'])\n    all_val_loss.extend(hist.history['val_loss'])\n\nplt.plot(epochs_range, all_train_loss, 'b-', label='Training Loss', alpha=0.8)\nplt.plot(epochs_range, all_val_loss, 'r-', label='Validation Loss', alpha=0.8)\nplt.title('TPU Loss History')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Metrics comparison\nplt.subplot(2, 3, 3)\nmetrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\nsingle_values = [final_accuracy, final_precision, final_recall, final_f1]\ntta_values = [tta_accuracy, tta_precision, tta_recall, tta_f1]\n\nx = np.arange(len(metrics))\nwidth = 0.35\n\nplt.bar(x - width/2, single_values, width, label='Single Model', alpha=0.8, color='skyblue')\nplt.bar(x + width/2, tta_values, width, label='TTA Model', alpha=0.8, color='lightgreen')\n\nplt.xlabel('Metrics')\nplt.ylabel('Score')\nplt.title('TPU Model Performance')\nplt.xticks(x, metrics)\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Confusion matrices comparison\nplt.subplot(2, 3, 4)\ncm_single = confusion_matrix(y_true_final, y_pred_final)\nsns.heatmap(cm_single, annot=True, fmt='d', cmap='Blues', \n           xticklabels=class_names, yticklabels=class_names)\nplt.title(f'Single Model\\nAccuracy: {final_accuracy:.4f}')\n\nplt.subplot(2, 3, 5)\ncm_tta = confusion_matrix(tta_y_true, tta_y_pred)\nsns.heatmap(cm_tta, annot=True, fmt='d', cmap='Greens', \n           xticklabels=class_names, yticklabels=class_names)\nplt.title(f'TTA Model\\nAccuracy: {tta_accuracy:.4f}')\n\n# Performance improvement\nplt.subplot(2, 3, 6)\nmodels = ['Single Model', 'TTA Model']\naccuracies = [final_accuracy, tta_accuracy]\ncolors = ['skyblue', 'lightgreen']\n\nbars = plt.bar(models, accuracies, color=colors, alpha=0.8, edgecolor='black')\nplt.title('TPU Model Accuracy Comparison')\nplt.ylabel('Accuracy')\nplt.ylim(0, 1)\n\nfor bar, acc in zip(bars, accuracies):\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n             f'{acc:.3f}\\n({acc*100:.1f}%)',\n             ha='center', va='bottom', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TPU MODEL TRAINING COMPLETE\")\nprint(\"=\"*60)\nprint(f\"Strategy used: {strategy}\")\nprint(f\"Number of replicas: {strategy.num_replicas_in_sync}\")\nprint(f\"Global batch size: {GLOBAL_BATCH_SIZE}\")\nprint(f\"Best Model Saved as: best_fine_tuned_tpu_model.h5\")\nprint(f\"Final Single Model Accuracy: {final_accuracy:.4f} ({final_accuracy*100:.2f}%)\")\nprint(f\"Final TTA Accuracy: {tta_accuracy:.4f} ({tta_accuracy*100:.2f}%)\")\nprint(f\"Total Improvement with TTA: {((tta_accuracy - final_accuracy) * 100):+.2f}%\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null}]}